{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/miniconda3/envs/dtiam-esm/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import get_train_test_datasets\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import json\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_model_types = ['NN_TORCH', 'FASTAI', 'GBM', 'XGB']\n",
    "hyperparameters = {\n",
    "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
    "\t'GBM': [\n",
    "        {},\n",
    "        {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
    "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
    "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}]\n",
    "}\n",
    "eval_metric = 'roc_auc'\n",
    "preset = None\n",
    "problem_type = 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# dataset = 'hetionet'\n",
    "dataset = 'yamanishi_08'\n",
    "setting = 'warm_start'\n",
    "# setting = 'drug_coldstart'\n",
    "# setting = 'protein_coldstart'\n",
    "target = 'dti'\n",
    "fold = 0\n",
    "mol_model = 'MolE_GuacaMol_27113.ckpt'\n",
    "# mol_model = 'BerMolModel_base.pkl'\n",
    "# mol_model = 'dtiam_kiba_compound_features'\n",
    "train_df, test_df = get_train_test_datasets(fold=fold, setting=setting, dataset=dataset, mol_model=mol_model, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50757, 2049)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       44.04 GB / 62.63 GB (70.3%)\n",
      "Disk Space Avail:   163.56 GB / 1684.96 GB (9.7%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (50757 samples, 416.21 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_0-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    50757\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_0-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    45046.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 396.54 MB (0.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t3.4s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 396.54 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.04925429004866324, Train Rows: 48257, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0891715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9708\t = Validation score   (roc_auc)\n",
      "\t79.2s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9776\t = Validation score   (roc_auc)\n",
      "\t56.34s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "/home/julian/miniconda3/envs/dtiam-esm/lib/python3.10/site-packages/xgboost/core.py:729: UserWarning: [15:18:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "\t0.9731\t = Validation score   (roc_auc)\n",
      "\t51.01s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9854\t = Validation score   (roc_auc)\n",
      "\t80.27s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9662\t = Validation score   (roc_auc)\n",
      "\t109.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 0.95, 'NeuralNetFastAI': 0.05}\n",
      "\t0.986\t = Validation score   (roc_auc)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 381.75s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 26561.6 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_0-MolE_GuacaMol_27113\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       38.10 GB / 62.63 GB (60.8%)\n",
      "Disk Space Avail:   163.10 GB / 1684.96 GB (9.7%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (50757 samples, 416.21 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_1-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    50757\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_1-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    38979.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 396.54 MB (1.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t3.2s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 396.54 MB (1.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.04925429004866324, Train Rows: 48257, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0942451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.965\t = Validation score   (roc_auc)\n",
      "\t105.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9821\t = Validation score   (roc_auc)\n",
      "\t40.99s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9728\t = Validation score   (roc_auc)\n",
      "\t13.59s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9841\t = Validation score   (roc_auc)\n",
      "\t38.93s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9678\t = Validation score   (roc_auc)\n",
      "\t105.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 0.556, 'NeuralNetFastAI': 0.333, 'LightGBM': 0.111}\n",
      "\t0.986\t = Validation score   (roc_auc)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 309.84s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 22756.2 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_1-MolE_GuacaMol_27113\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       37.80 GB / 62.63 GB (60.4%)\n",
      "Disk Space Avail:   162.70 GB / 1684.96 GB (9.7%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (50757 samples, 416.21 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_2-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    50757\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_2-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    38726.23 MB\n",
      "\tTrain Data (Original)  Memory Usage: 396.54 MB (1.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t3.3s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 396.54 MB (1.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.04925429004866324, Train Rows: 48257, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.098083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9636\t = Validation score   (roc_auc)\n",
      "\t70.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 7: early stopping\n",
      "\t0.9778\t = Validation score   (roc_auc)\n",
      "\t41.19s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9692\t = Validation score   (roc_auc)\n",
      "\t12.13s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9792\t = Validation score   (roc_auc)\n",
      "\t38.54s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.965\t = Validation score   (roc_auc)\n",
      "\t90.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 0.529, 'NeuralNetFastAI': 0.471}\n",
      "\t0.9817\t = Validation score   (roc_auc)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 258.0s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 25923.9 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_2-MolE_GuacaMol_27113\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       33.69 GB / 62.63 GB (53.8%)\n",
      "Disk Space Avail:   162.29 GB / 1684.96 GB (9.6%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (50757 samples, 416.21 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_3-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    50757\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_3-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    34499.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 396.54 MB (1.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t3.2s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 396.54 MB (1.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.04925429004866324, Train Rows: 48257, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0851434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9812\t = Validation score   (roc_auc)\n",
      "\t90.8s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9771\t = Validation score   (roc_auc)\n",
      "\t41.17s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.983\t = Validation score   (roc_auc)\n",
      "\t13.05s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9876\t = Validation score   (roc_auc)\n",
      "\t64.48s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9849\t = Validation score   (roc_auc)\n",
      "\t141.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMLarge': 0.609, 'NeuralNetTorch': 0.391}\n",
      "\t0.9894\t = Validation score   (roc_auc)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 356.33s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 34491.3 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_3-MolE_GuacaMol_27113\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       33.36 GB / 62.63 GB (53.3%)\n",
      "Disk Space Avail:   161.88 GB / 1684.96 GB (9.6%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (50757 samples, 416.21 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_4-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    50757\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_4-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    34500.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 396.54 MB (1.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t3.3s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 396.54 MB (1.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.04925429004866324, Train Rows: 48257, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0957985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9653\t = Validation score   (roc_auc)\n",
      "\t79.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 8: early stopping\n",
      "\t0.978\t = Validation score   (roc_auc)\n",
      "\t43.05s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9695\t = Validation score   (roc_auc)\n",
      "\t15.75s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9886\t = Validation score   (roc_auc)\n",
      "\t163.3s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.969\t = Validation score   (roc_auc)\n",
      "\t109.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t0.9886\t = Validation score   (roc_auc)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 416.37s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 47868.6 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/yamanishi_08-warm_start-fold_4-MolE_GuacaMol_27113\")\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    dataset_path = \"../data/dta/\" + dataset\n",
    "    folds_path = dataset_path + f\"/data_folds/{setting}/\"\n",
    "\n",
    "    train_df, test_df = get_train_test_datasets(fold=fold, setting=setting, dataset=dataset, mol_model=mol_model, target=target)\n",
    "    \n",
    "    model_path = f\"/home/julian/DTIAM/code/AutogluonModels/{dataset}-{setting}-fold_{fold}-{mol_model.split('.')[0]}\"\n",
    "    print(model_path)\n",
    "\n",
    "    predictor = TabularPredictor(path=model_path, label=\"y\", eval_metric=eval_metric, problem_type=problem_type).fit(\n",
    "        train_data=train_df, included_model_types=included_model_types, presets=preset, hyperparameters=hyperparameters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag-20250616_230514 es drug_coldstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.path = mol_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_nolab = test_df.drop(columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores = predictor.predict(test_data_nolab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import load_data, rmse, mse, pearson, spearman, ci, roc_auc, pr_auc\n",
    "res_all = pd.DataFrame(columns=[\"RMSE\", \"MSE\", \"Pearson\", \"Spearman\", \"CI\"])\n",
    "G, P = np.array(test_df[\"y\"]), np.array(pred_scores)\n",
    "ret = [rmse(G, P), mse(G, P), pearson(G, P), spearman(G, P), ci(G, P)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, RMSE: 0.5849176988608807, MSE: 0.342128714440708, Pearson: 0.7199550158109927, Spearman: 0.638433401990211, CI: 0.7514334587237242\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(\n",
    "    f\"fold: {i+1}, RMSE: {ret[0]}, MSE: {ret[1]}, Pearson: {ret[2]}, Spearman: {ret[3]}, CI: {ret[4]}\"\n",
    ")\n",
    "res_all.loc[i] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtiam-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
