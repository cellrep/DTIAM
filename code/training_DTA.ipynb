{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/miniconda3/envs/dtiam-esm/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import load_data\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import json\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "from utils import get_train_test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_model_types = ['NN_TORCH', 'FASTAI', 'GBM', 'XGB']\n",
    "hyperparameters = {\n",
    "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
    "\t'GBM': [\n",
    "        {'ag_args_fit': {'num_gpus': 1}},\n",
    "        {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'ag_args_fit': {'num_gpus': 1}, 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
    "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
    "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}]\n",
    "}\n",
    "eval_metric = 'rmse'\n",
    "preset=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       27.35 GB / 62.63 GB (43.7%)\n",
      "Disk Space Avail:   161.44 GB / 1684.96 GB (9.6%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (94603 samples, 727.31 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/kiba-warm_start-fold_1-MolE_GuacaMol_27113-esmc_600m\"\n",
      "Train Data Rows:    94603\n",
      "Train Data Columns: 1920\n",
      "Label Column:       y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    27979.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 692.89 MB (2.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1920 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1920 | ['0', '1', '2', '3', '4', ...]\n",
      "\t6.2s = Fit runtime\n",
      "\t1920 features in original data used to generate 1920 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 692.89 MB (2.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 8.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.026426223269875163, Train Rows: 92103, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{'ag_args_fit': {'num_gpus': 1}}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'ag_args_fit': {'num_gpus': 1}, 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "WARNING: Unknown ag_args key: ag_args_fit\n",
      "Fitting 5 L1 models, fit_strategy=\"parallel\" ...\n",
      "WARNING: fit_strategy='parallel', but `num_gpus=1` is specified. GPU is not yet supported for `parallel` fit_strategy. To enable parallel, ensure you specify `num_gpus=0` in the fit call. Falling back to fit_strategy='sequential' ... \n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    }
   ],
   "source": [
    "dataset = 'kiba'\n",
    "setting = 'warm_start'\n",
    "target = 'dta'\n",
    "# setting = 'drug_coldstart'\n",
    "# setting = 'protein_coldstart'\n",
    "for fold in range(1, 5):\n",
    "    dataset_path = \"../data/dta/\" + dataset\n",
    "    folds_path = dataset_path + f\"/data_folds/{setting}/\"\n",
    "\n",
    "    mol_model = 'MolE_GuacaMol_27113.ckpt'\n",
    "    prot_model = 'esmc_600m'\n",
    "    # mol_model = 'dtiam_kiba_compound_features'\n",
    "    train_df, test_df = get_train_test_datasets(fold=fold, setting=setting, dataset=dataset, target=target, mol_model=mol_model, prot_model=prot_model)\n",
    "    model_path = f\"/home/julian/DTIAM/code/AutogluonModels/{dataset}-{setting}-fold_{fold}-{mol_model.split('.')[0]}-{prot_model}\"\n",
    "    \n",
    "    predictor = TabularPredictor(path=model_path, label=\"y\", eval_metric=eval_metric, problem_type='regression').fit(\n",
    "        train_data=train_df, included_model_types=included_model_types, presets=preset, hyperparameters=hyperparameters, fit_strategy='parallel'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag-20250616_230514 es drug_coldstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.path = mol_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_nolab = test_df.drop(columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores = predictor.predict(test_data_nolab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import load_data, rmse, mse, pearson, spearman, ci, roc_auc, pr_auc\n",
    "res_all = pd.DataFrame(columns=[\"RMSE\", \"MSE\", \"Pearson\", \"Spearman\", \"CI\"])\n",
    "G, P = np.array(test_df[\"y\"]), np.array(pred_scores)\n",
    "ret = [rmse(G, P), mse(G, P), pearson(G, P), spearman(G, P), ci(G, P)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, RMSE: 0.5849176988608807, MSE: 0.342128714440708, Pearson: 0.7199550158109927, Spearman: 0.638433401990211, CI: 0.7514334587237242\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(\n",
    "    f\"fold: {i+1}, RMSE: {ret[0]}, MSE: {ret[1]}, Pearson: {ret[2]}, Spearman: {ret[3]}, CI: {ret[4]}\"\n",
    ")\n",
    "res_all.loc[i] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtiam-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
