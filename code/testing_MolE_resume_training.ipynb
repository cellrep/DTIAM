{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/miniconda3/envs/dtiam-esm/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import load_data\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_embeddings(file_path, unique_keys=None):\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        data = f[\"data\"][()]\n",
    "        keys = [k.decode(\"utf-8\") for k in f[\"keys\"][()]]\n",
    "        data_dict_loaded = dict(zip(keys, data))\n",
    "    if unique_keys is not None:\n",
    "        new_dict = {key: data_dict_loaded[key] for key in unique_keys}\n",
    "        return new_dict\n",
    "    return data_dict_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'kiba'\n",
    "# setting = 'warm_start'\n",
    "setting = 'drug_coldstart'\n",
    "fold = 0\n",
    "dataset_path = \"../data/dta/\" + dataset\n",
    "folds_path = dataset_path + f\"/data_folds/{setting}/\"\n",
    "# fpath = f'/home/julian/DTIAM/data/dta/{dataset}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# comp_feat = pickle.load(open(dataset_path + \"/features/compound_features.pkl\", \"rb\"))\n",
    "prot_feat = pickle.load(open(dataset_path + \"/features/protein_features.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(folds_path + \"train_fold_\" + str(fold) + \".csv\")\n",
    "test = pd.read_csv(folds_path + \"test_fold_\" + str(fold) + \".csv\")\n",
    "\n",
    "ligands = json.load(open(dataset_path + \"/ligands_can.txt\"))\n",
    "proteins = json.load(open(dataset_path + \"/proteins.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['smiles'] = train['drug_id'].apply(lambda x: ligands[x])\n",
    "test['smiles'] = test['drug_id'].apply(lambda x: ligands[x])\n",
    "MolE_embs = load_embeddings('/home/julian/mole_embed/notebooks/DTI_benchmark/prot_mols_embeddings/MolE_GuacaMol_27113.ckpt.h5')\n",
    "train['drub_emb'] = train['smiles'].apply(lambda x: MolE_embs[x])\n",
    "test['drub_emb'] = test['smiles'].apply(lambda x: MolE_embs[x])\n",
    "train.drop(columns=['drug_id', 'smiles'], inplace=True)\n",
    "test.drop(columns=['drug_id', 'smiles'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['prot_emb'] = train['protein_id'].apply(lambda x: prot_feat[x])\n",
    "test['prot_emb'] = test['protein_id'].apply(lambda x: prot_feat[x])\n",
    "train.drop(columns=['protein_id'], inplace=True)\n",
    "test.drop(columns=['protein_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_df = train.apply(lambda x: pd.Series(np.hstack([x['drub_emb'], x['prot_emb']])), axis=1)\n",
    "train_df['y'] = train['affinity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test.apply(lambda x: pd.Series(np.hstack([x['drub_emb'], x['prot_emb']])), axis=1)\n",
    "test_df['y'] = test['affinity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.486857</td>\n",
       "      <td>-2.852608</td>\n",
       "      <td>2.226141</td>\n",
       "      <td>-0.771469</td>\n",
       "      <td>-0.53247</td>\n",
       "      <td>1.374549</td>\n",
       "      <td>-0.905905</td>\n",
       "      <td>2.068331</td>\n",
       "      <td>-2.270883</td>\n",
       "      <td>-0.709579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003497</td>\n",
       "      <td>-0.092343</td>\n",
       "      <td>0.056811</td>\n",
       "      <td>-0.017626</td>\n",
       "      <td>-0.057044</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>-0.113871</td>\n",
       "      <td>0.059128</td>\n",
       "      <td>0.047931</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.486857</td>\n",
       "      <td>-2.852608</td>\n",
       "      <td>2.226141</td>\n",
       "      <td>-0.771469</td>\n",
       "      <td>-0.53247</td>\n",
       "      <td>1.374549</td>\n",
       "      <td>-0.905905</td>\n",
       "      <td>2.068331</td>\n",
       "      <td>-2.270883</td>\n",
       "      <td>-0.709579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030705</td>\n",
       "      <td>-0.091796</td>\n",
       "      <td>-0.016181</td>\n",
       "      <td>0.029888</td>\n",
       "      <td>-0.057828</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>-0.059174</td>\n",
       "      <td>0.046706</td>\n",
       "      <td>0.080474</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.486857</td>\n",
       "      <td>-2.852608</td>\n",
       "      <td>2.226141</td>\n",
       "      <td>-0.771469</td>\n",
       "      <td>-0.53247</td>\n",
       "      <td>1.374549</td>\n",
       "      <td>-0.905905</td>\n",
       "      <td>2.068331</td>\n",
       "      <td>-2.270883</td>\n",
       "      <td>-0.709579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028138</td>\n",
       "      <td>-0.069517</td>\n",
       "      <td>0.017836</td>\n",
       "      <td>0.021373</td>\n",
       "      <td>-0.023830</td>\n",
       "      <td>0.049003</td>\n",
       "      <td>-0.069897</td>\n",
       "      <td>0.045682</td>\n",
       "      <td>0.043708</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.486857</td>\n",
       "      <td>-2.852608</td>\n",
       "      <td>2.226141</td>\n",
       "      <td>-0.771469</td>\n",
       "      <td>-0.53247</td>\n",
       "      <td>1.374549</td>\n",
       "      <td>-0.905905</td>\n",
       "      <td>2.068331</td>\n",
       "      <td>-2.270883</td>\n",
       "      <td>-0.709579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006736</td>\n",
       "      <td>-0.179605</td>\n",
       "      <td>-0.001505</td>\n",
       "      <td>0.028528</td>\n",
       "      <td>-0.114054</td>\n",
       "      <td>0.034848</td>\n",
       "      <td>-0.234336</td>\n",
       "      <td>0.080385</td>\n",
       "      <td>0.143831</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.486857</td>\n",
       "      <td>-2.852608</td>\n",
       "      <td>2.226141</td>\n",
       "      <td>-0.771469</td>\n",
       "      <td>-0.53247</td>\n",
       "      <td>1.374549</td>\n",
       "      <td>-0.905905</td>\n",
       "      <td>2.068331</td>\n",
       "      <td>-2.270883</td>\n",
       "      <td>-0.709579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037864</td>\n",
       "      <td>-0.255422</td>\n",
       "      <td>0.016660</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>-0.127575</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>-0.200034</td>\n",
       "      <td>0.106770</td>\n",
       "      <td>0.148296</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2049 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3        4         5         6  \\\n",
       "0 -1.486857 -2.852608  2.226141 -0.771469 -0.53247  1.374549 -0.905905   \n",
       "1 -1.486857 -2.852608  2.226141 -0.771469 -0.53247  1.374549 -0.905905   \n",
       "2 -1.486857 -2.852608  2.226141 -0.771469 -0.53247  1.374549 -0.905905   \n",
       "3 -1.486857 -2.852608  2.226141 -0.771469 -0.53247  1.374549 -0.905905   \n",
       "4 -1.486857 -2.852608  2.226141 -0.771469 -0.53247  1.374549 -0.905905   \n",
       "\n",
       "          7         8         9  ...      2039      2040      2041      2042  \\\n",
       "0  2.068331 -2.270883 -0.709579  ... -0.003497 -0.092343  0.056811 -0.017626   \n",
       "1  2.068331 -2.270883 -0.709579  ...  0.030705 -0.091796 -0.016181  0.029888   \n",
       "2  2.068331 -2.270883 -0.709579  ...  0.028138 -0.069517  0.017836  0.021373   \n",
       "3  2.068331 -2.270883 -0.709579  ... -0.006736 -0.179605 -0.001505  0.028528   \n",
       "4  2.068331 -2.270883 -0.709579  ...  0.037864 -0.255422  0.016660  0.003467   \n",
       "\n",
       "       2043      2044      2045      2046      2047     y  \n",
       "0 -0.057044  0.038800 -0.113871  0.059128  0.047931  11.1  \n",
       "1 -0.057828  0.048171 -0.059174  0.046706  0.080474  11.1  \n",
       "2 -0.023830  0.049003 -0.069897  0.045682  0.043708  11.1  \n",
       "3 -0.114054  0.034848 -0.234336  0.080385  0.143831  11.1  \n",
       "4 -0.127575  0.044249 -0.200034  0.106770  0.148296  11.1  \n",
       "\n",
       "[5 rows x 2049 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_model_types = []\n",
    "eval_metric = None\n",
    "preset=None\n",
    "ex_model=[\n",
    "    # 'RandomForestMSE', 'KNeighborsDist', 'KNeighborsUnif', 'CatBoost'\n",
    "]\n",
    "# predictor = TabularPredictor(label=\"y\", eval_metric=eval_metric).fit(\n",
    "#     train_data=train_df, excluded_model_types=ex_model, presets=preset\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag-20250616_230514 es drug_coldstart\n",
    "predictor = TabularPredictor.load('/home/julian/DTIAM/code/AutogluonModels/ag-drug_cold_start_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CatBoost',\n",
       " 'ExtraTreesMSE',\n",
       " 'KNeighborsDist',\n",
       " 'KNeighborsUnif',\n",
       " 'LightGBM',\n",
       " 'LightGBMXT',\n",
       " 'NeuralNetFastAI',\n",
       " 'RandomForestMSE',\n",
       " 'XGBoost']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted(predictor.model_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Predictor is already fit! To fit additional models, refer to `predictor.fit_extra`, or create a new `Predictor`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcluded_model_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mex_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dtiam-esm/lib/python3.10/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dtiam-esm/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:1042\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03mFit models to predict a column of a data table (label) based on the other columns (features).\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m>>> predictor = TabularPredictor(label=label, eval_metric=eval_metric).fit(train_data, presets=['best_quality'], time_limit=time_limit)\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fit:\n\u001b[0;32m-> 1042\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictor is already fit! To fit additional models, refer to `predictor.fit_extra`, or create a new `Predictor`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1044\u001b[0m verbosity \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity)\n\u001b[1;32m   1045\u001b[0m set_logger_verbosity(verbosity)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Predictor is already fit! To fit additional models, refer to `predictor.fit_extra`, or create a new `Predictor`."
     ]
    }
   ],
   "source": [
    "predictor.fit(train_data=train_df, excluded_model_types=ex_model, presets=preset, resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_nolab = test_df.drop(columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores = predictor.predict(test_data_nolab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import load_data, rmse, mse, pearson, spearman, ci, roc_auc, pr_auc\n",
    "res_all = pd.DataFrame(columns=[\"RMSE\", \"MSE\", \"Pearson\", \"Spearman\", \"CI\"])\n",
    "G, P = np.array(test_df[\"y\"]), np.array(pred_scores)\n",
    "ret = [rmse(G, P), mse(G, P), pearson(G, P), spearman(G, P), ci(G, P)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, RMSE: 0.376717739035863, MSE: 0.14191625490429255, Pearson: 0.8933051324301862, Spearman: 0.8893694248450851, CI: 0.890743195275649\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(\n",
    "    f\"fold: {i+1}, RMSE: {ret[0]}, MSE: {ret[1]}, Pearson: {ret[2]}, Spearman: {ret[3]}, CI: {ret[4]}\"\n",
    ")\n",
    "res_all.loc[i] = ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtiam-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
