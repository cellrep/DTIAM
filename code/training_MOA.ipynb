{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/miniconda3/envs/dtiam-esm/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import get_train_test_datasets\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_model_types = ['NN_TORCH', 'FASTAI', 'GBM', 'XGB']\n",
    "hyperparameters = {\n",
    "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
    "\t'GBM': [\n",
    "        {'ag_args_fit': {'num_gpus': 1}},\n",
    "        {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'ag_args_fit': {'num_gpus': 1}, 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
    "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
    "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}]\n",
    "}\n",
    "eval_metric = 'roc_auc'\n",
    "preset = None\n",
    "problem_type = 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# dataset = 'activation'\n",
    "dataset = 'inhibition'\n",
    "setting = 'warm_start'\n",
    "# setting = 'drug_coldstart'\n",
    "# setting = 'protein_coldstart'\n",
    "target = 'moa'\n",
    "fold = 0\n",
    "mol_model = 'MolE_GuacaMol_27113.ckpt'\n",
    "# mol_model = 'BerMolModel_base.pkl'\n",
    "# mol_model = 'dtiam_kiba_compound_features'\n",
    "train_df, test_df = get_train_test_datasets(fold=fold, setting=setting, dataset=dataset, mol_model=mol_model, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       26.72 GB / 62.63 GB (42.7%)\n",
      "Disk Space Avail:   192.88 GB / 1684.96 GB (11.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (185284 samples, 1519.33 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_0-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    185284\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_0-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    27668.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1447.53 MB (5.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 5.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t17.2s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1447.53 MB (5.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 26.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01349280024179098, Train Rows: 182784, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{'ag_args_fit': {'num_gpus': 1}}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'ag_args_fit': {'num_gpus': 1}, 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "WARNING: Unknown ag_args key: ag_args_fit\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.122963\n",
      "[2000]\tvalid_set's binary_logloss: 0.108273\n",
      "[3000]\tvalid_set's binary_logloss: 0.105907\n",
      "[4000]\tvalid_set's binary_logloss: 0.107039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9699\t = Validation score   (roc_auc)\n",
      "\t2271.77s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 14.988 GB out of 19.010 GB available memory (78.843%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.10 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\t0.971\t = Validation score   (roc_auc)\n",
      "\t370.85s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "/home/julian/miniconda3/envs/dtiam-esm/lib/python3.10/site-packages/xgboost/core.py:729: UserWarning: [22:24:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "\t0.9686\t = Validation score   (roc_auc)\n",
      "\t71.75s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9743\t = Validation score   (roc_auc)\n",
      "\t315.86s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.110045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9694\t = Validation score   (roc_auc)\n",
      "\t629.34s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 0.467, 'NeuralNetFastAI': 0.267, 'NeuralNetTorch': 0.2, 'LightGBMLarge': 0.067}\n",
      "\t0.98\t = Validation score   (roc_auc)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3690.36s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 8146.8 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_0-MolE_GuacaMol_27113\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       19.77 GB / 62.63 GB (31.6%)\n",
      "Disk Space Avail:   190.51 GB / 1684.96 GB (11.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (185284 samples, 1519.33 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_1-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    185284\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_1-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    21721.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1447.53 MB (6.7% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 6.7% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t8.1s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1447.53 MB (6.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 12.86s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01349280024179098, Train Rows: 182784, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{'ag_args_fit': {'num_gpus': 1}}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'ag_args_fit': {'num_gpus': 1}, 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "WARNING: Unknown ag_args key: ag_args_fit\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.130263\n",
      "[2000]\tvalid_set's binary_logloss: 0.116048\n",
      "[3000]\tvalid_set's binary_logloss: 0.114102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9621\t = Validation score   (roc_auc)\n",
      "\t170.65s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 14.988 GB out of 18.911 GB available memory (79.258%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.11 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\t0.9636\t = Validation score   (roc_auc)\n",
      "\t171.98s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.962\t = Validation score   (roc_auc)\n",
      "\t51.15s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.968\t = Validation score   (roc_auc)\n",
      "\t332.61s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.11203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9644\t = Validation score   (roc_auc)\n",
      "\t4387.27s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.474, 'NeuralNetTorch': 0.368, 'LightGBMLarge': 0.158}\n",
      "\t0.9724\t = Validation score   (roc_auc)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5128.62s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10802.3 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_1-MolE_GuacaMol_27113\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.23 GB / 62.63 GB (25.9%)\n",
      "Disk Space Avail:   189.06 GB / 1684.96 GB (11.2%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (185284 samples, 1519.33 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_2-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    185284\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_2-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18193.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1447.53 MB (8.0% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 8.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t11.2s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1447.53 MB (8.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 19.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01349280024179098, Train Rows: 182784, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{'ag_args_fit': {'num_gpus': 1}}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'ag_args_fit': {'num_gpus': 1}, 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "WARNING: Unknown ag_args key: ag_args_fit\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.119929\n",
      "[2000]\tvalid_set's binary_logloss: 0.10322\n",
      "[3000]\tvalid_set's binary_logloss: 0.0995113\n",
      "[4000]\tvalid_set's binary_logloss: 0.101438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9657\t = Validation score   (roc_auc)\n",
      "\t1971.34s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 14.988 GB out of 17.630 GB available memory (85.016%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.18 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\t0.9754\t = Validation score   (roc_auc)\n",
      "\t422.11s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.966\t = Validation score   (roc_auc)\n",
      "\t70.78s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9818\t = Validation score   (roc_auc)\n",
      "\t1250.94s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.106122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9583\t = Validation score   (roc_auc)\n",
      "\t660.22s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 0.955, 'NeuralNetFastAI': 0.045}\n",
      "\t0.9837\t = Validation score   (roc_auc)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4397.85s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 15461.0 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_2-MolE_GuacaMol_27113\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       15.52 GB / 62.63 GB (24.8%)\n",
      "Disk Space Avail:   186.68 GB / 1684.96 GB (11.1%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (185284 samples, 1519.33 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_3-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    185284\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_3-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17375.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1447.53 MB (8.3% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 8.3% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t8.3s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1447.53 MB (8.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 13.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01349280024179098, Train Rows: 182784, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{'ag_args_fit': {'num_gpus': 1}}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'ag_args_fit': {'num_gpus': 1}, 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "WARNING: Unknown ag_args key: ag_args_fit\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.125181\n",
      "[2000]\tvalid_set's binary_logloss: 0.108741\n",
      "[3000]\tvalid_set's binary_logloss: 0.105407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9627\t = Validation score   (roc_auc)\n",
      "\t168.26s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 14.988 GB out of 17.044 GB available memory (87.937%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.22 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\t0.9792\t = Validation score   (roc_auc)\n",
      "\t357.41s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9708\t = Validation score   (roc_auc)\n",
      "\t74.23s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9767\t = Validation score   (roc_auc)\n",
      "\t496.53s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.101543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9676\t = Validation score   (roc_auc)\n",
      "\t4887.73s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.364, 'NeuralNetTorch': 0.318, 'XGBoost': 0.273, 'LightGBMLarge': 0.045}\n",
      "\t0.9818\t = Validation score   (roc_auc)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5999.7s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4770.0 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_3-MolE_GuacaMol_27113\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drug_id', 'protein_id', 'affinity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          20\n",
      "Memory Avail:       15.62 GB / 62.63 GB (24.9%)\n",
      "Disk Space Avail:   184.37 GB / 1684.96 GB (10.9%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (185284 samples, 1519.33 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_4-MolE_GuacaMol_27113\"\n",
      "Train Data Rows:    185284\n",
      "Train Data Columns: 2048\n",
      "Label Column:       y\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_4-MolE_GuacaMol_27113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17637.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1447.53 MB (8.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 8.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 2048 | ['0', '1', '2', '3', '4', ...]\n",
      "\t14.3s = Fit runtime\n",
      "\t2048 features in original data used to generate 2048 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1447.53 MB (8.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 21.84s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01349280024179098, Train Rows: 182784, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'GBM': [{'ag_args_fit': {'num_gpus': 1}}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'ag_args_fit': {'num_gpus': 1}, 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'FASTAI': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "Included models: ['NN_TORCH', 'FASTAI', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "WARNING: Unknown ag_args key: ag_args_fit\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.130524\n",
      "[2000]\tvalid_set's binary_logloss: 0.114027\n",
      "[3000]\tvalid_set's binary_logloss: 0.112611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9671\t = Validation score   (roc_auc)\n",
      "\t1652.04s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 14.988 GB out of 17.433 GB available memory (85.975%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.20 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\t0.9677\t = Validation score   (roc_auc)\n",
      "\t241.36s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9711\t = Validation score   (roc_auc)\n",
      "\t74.48s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9754\t = Validation score   (roc_auc)\n",
      "\t453.34s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.113336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9676\t = Validation score   (roc_auc)\n",
      "\t677.68s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 0.632, 'NeuralNetFastAI': 0.263, 'XGBoost': 0.105}\n",
      "\t0.9781\t = Validation score   (roc_auc)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3123.84s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 11896.5 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/julian/DTIAM/code/AutogluonModels/inhibition-warm_start-fold_4-MolE_GuacaMol_27113\")\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    dataset_path = \"../data/dta/\" + dataset\n",
    "    folds_path = dataset_path + f\"/data_folds/{setting}/\"\n",
    "\n",
    "    train_df, test_df = get_train_test_datasets(fold=fold, setting=setting, dataset=dataset, mol_model=mol_model, target=target)\n",
    "    \n",
    "    model_path = f\"/home/julian/DTIAM/code/AutogluonModels/{dataset}-{setting}-fold_{fold}-{mol_model.split('.')[0]}\"\n",
    "    print(model_path)\n",
    "\n",
    "    predictor = TabularPredictor(path=model_path, label=\"y\", eval_metric=eval_metric, problem_type=problem_type).fit(\n",
    "        train_data=train_df, included_model_types=included_model_types, presets=preset, hyperparameters=hyperparameters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag-20250616_230514 es drug_coldstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.path = mol_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_nolab = test_df.drop(columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores = predictor.predict(test_data_nolab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import load_data, rmse, mse, pearson, spearman, ci, roc_auc, pr_auc\n",
    "res_all = pd.DataFrame(columns=[\"RMSE\", \"MSE\", \"Pearson\", \"Spearman\", \"CI\"])\n",
    "G, P = np.array(test_df[\"y\"]), np.array(pred_scores)\n",
    "ret = [rmse(G, P), mse(G, P), pearson(G, P), spearman(G, P), ci(G, P)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, RMSE: 0.18102848733628069, MSE: 0.032771313227261935, Pearson: 0.798958836937087, Spearman: 0.798958836937095, CI: 0.891639104075121\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(\n",
    "    f\"fold: {i+1}, RMSE: {ret[0]}, MSE: {ret[1]}, Pearson: {ret[2]}, Spearman: {ret[3]}, CI: {ret[4]}\"\n",
    ")\n",
    "res_all.loc[i] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtiam-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
